[
    {
        "id": "63608c8ffd25253a",
        "type": "subflow",
        "name": "llm:Model",
        "info": "\r\nThis node build and cache Azure OpenAI Client.\r\n\r\n# Input\r\n\r\nModel's credential should be provide using environment mechanism. \r\nSo all the custom variable related to the project will be stored in one location.\r\nYou can use a pattern like `env-llm-gpt4-key` for you env variables.\r\n\r\n- Model: the name of the Model\r\n- Key: The Azure API Key\r\n- Endpoint : The Azure API Endpoint\r\n\r\n# Output\r\n\r\nThe model is return into `msg.llm.model`\r\nThe model is cached at Node level.",
        "category": "Encausse.net",
        "in": [
            {
                "x": 80,
                "y": 100,
                "wires": [
                    {
                        "id": "f8484d1c8d4ce3da"
                    }
                ]
            }
        ],
        "out": [
            {
                "x": 410,
                "y": 60,
                "wires": [
                    {
                        "id": "f8484d1c8d4ce3da",
                        "port": 0
                    }
                ]
            },
            {
                "x": 460,
                "y": 120,
                "wires": [
                    {
                        "id": "f8484d1c8d4ce3da",
                        "port": 1
                    }
                ]
            }
        ],
        "env": [
            {
                "name": "env-llm-model",
                "type": "env",
                "value": "env-llm-model",
                "ui": {
                    "icon": "font-awesome/fa-cogs",
                    "label": {
                        "en-US": "Model*"
                    },
                    "type": "input",
                    "opts": {
                        "types": [
                            "str",
                            "env"
                        ]
                    }
                }
            },
            {
                "name": "env-llm-key",
                "type": "env",
                "value": "env-llm-key",
                "ui": {
                    "icon": "font-awesome/fa-user-secret",
                    "label": {
                        "en-US": "Key*"
                    },
                    "type": "input",
                    "opts": {
                        "types": [
                            "str",
                            "env"
                        ]
                    }
                }
            },
            {
                "name": "env-llm-endpoint",
                "type": "env",
                "value": "env-llm-endpoint",
                "ui": {
                    "icon": "font-awesome/fa-user-secret",
                    "label": {
                        "en-US": "Endpoint*"
                    },
                    "type": "input",
                    "opts": {
                        "types": [
                            "str",
                            "env"
                        ]
                    }
                }
            }
        ],
        "meta": {
            "module": "node-red-contrib-moonshots-llm",
            "type": "moonshots-llm-model",
            "version": "1.0.0",
            "author": "Jean-Philippe Encausse <Jp@encausse.net>",
            "desc": "Config Node of Azure OpenAI",
            "keywords": "Moonshots, NodeRED, LLM, OpenAI",
            "license": "MIT"
        },
        "color": "#9ff6fd",
        "outputLabels": [
            "Error",
            "Build msg.llm.model"
        ],
        "icon": "node-red/cog.svg"
    },
    {
        "id": "f8484d1c8d4ce3da",
        "type": "function",
        "z": "63608c8ffd25253a",
        "name": "Build Model",
        "func": "let MOONSHOTS = global.get('__MOONSHOTS__') || {}\nlet CONTEXT = MOONSHOTS['llm-model'] = MOONSHOTS['llm-model'] || {}\n\nmsg.llm = msg.llm || {}\n\nlet key      = MOONSHOTS.UTIL.getMessageProperty(msg, env.get(\"env-llm-key\"))\nlet endpoint = MOONSHOTS.UTIL.getMessageProperty(msg, env.get(\"env-llm-endpoint\"))\nlet id       = MOONSHOTS.UTIL.getMessageProperty(msg, env.get(\"env-llm-model\"))\n\nif (!key || !endpoint || !id){\n    msg.llm.error = 'Missing OpenAI credential'\n    return [msg, undefined];\n}\n\n// Retrive Subflow Context\nlet config    = CONTEXT[id]\n\n// Retrieve and/or build a configuration file\nif (!config || !config.client){\n    const { OpenAIClient, AzureKeyCredential } = azureOpenai;\n\n    config = { key, endpoint, id }\n    config.client = new OpenAIClient(config.endpoint, new AzureKeyCredential(config.key));\n    CONTEXT[id] = config;\n    global.set('__MOONSHOTS__', MOONSHOTS)\n}\n\nif (config.client === undefined){\n    msg.llm.error = 'Missing OpenAI client'\n    return [msg, undefined];\n}\n\nmsg.llm.model = config\nreturn node.send([undefined, msg],false);",
        "outputs": 2,
        "timeout": 0,
        "noerr": 0,
        "initialize": "",
        "finalize": "",
        "libs": [
            {
                "var": "azureOpenai",
                "module": "@azure/openai"
            }
        ],
        "x": 230,
        "y": 100,
        "wires": [
            [],
            []
        ],
        "icon": "node-red/link-out.svg"
    },
    {
        "id": "f36dcac84e162f71",
        "type": "subflow",
        "name": "llm:OpenAI",
        "info": "This node is a wrapper of Azure OpenAI Client.\r\n\r\n# Input\r\n\r\nModel's credential should be provide using environment mechanism. See llm:Model documentation.\r\n\r\n- User : A string of the user's query\r\n- Context: A string with some context (It will generate chat messsage, after System message)\r\n- History: An array of previous chat message\r\n- Tools: An array of tool (JSON) used by OpenAI\r\n\r\n# Output\r\n\r\nThe node will output a `msg.llm` object with all underlaying elements\r\n 1. Errors (look into msg.llm.error)\r\n 2. Tools to handle\r\n 3. Response\r\n\r\n# Tools\r\n\r\nTools JSON format must follow Azure OAI documentation.\r\n```\r\nconst tool = {\r\n    name: \"LLM_Tool_Name\",\r\n    description: \"Tool descirption\",\r\n    parameters: {\r\n        type: \"object\",\r\n        properties: {\r\n            query: {\r\n                type: \"string\",\r\n                description: \"Tool field description\",\r\n            },\r\n        },\r\n        required: [\"query\"],\r\n    },\r\n};\r\nmsg.llm.tools = msg.llm.tools || []\r\nmsg.llm.tools.push({ type: \"function\", function: tool })\r\n```\r\n\r\n\r\nYou should set a Switch node that look into `llm.tool.function.name`\r\nthen your code MUST call a callback function `msg.llm.toolCallback(string_result)`;",
        "category": "Encausse.net",
        "in": [
            {
                "x": 60,
                "y": 100,
                "wires": [
                    {
                        "id": "38daa06fe33eeb29"
                    }
                ]
            }
        ],
        "out": [
            {
                "x": 1090,
                "y": 40,
                "wires": [
                    {
                        "id": "a17776677d47af2d",
                        "port": 0
                    }
                ]
            },
            {
                "x": 1100,
                "y": 200,
                "wires": [
                    {
                        "id": "709d8be2580c8069",
                        "port": 1
                    }
                ]
            },
            {
                "x": 1080,
                "y": 260,
                "wires": [
                    {
                        "id": "183522ee54d78303",
                        "port": 0
                    }
                ]
            }
        ],
        "env": [
            {
                "name": "env-llm-model",
                "type": "env",
                "value": "env-llm-gpt-model",
                "ui": {
                    "icon": "font-awesome/fa-cogs",
                    "label": {
                        "en-US": "Model*"
                    },
                    "type": "input",
                    "opts": {
                        "types": [
                            "str",
                            "env"
                        ]
                    }
                }
            },
            {
                "name": "env-llm-key",
                "type": "env",
                "value": "env-llm-gpt-key",
                "ui": {
                    "icon": "font-awesome/fa-user-secret",
                    "label": {
                        "en-US": "Key*"
                    },
                    "type": "input",
                    "opts": {
                        "types": [
                            "str",
                            "env",
                            "cred"
                        ]
                    }
                }
            },
            {
                "name": "env-llm-endpoint",
                "type": "env",
                "value": "env-llm-gpt-endpoint",
                "ui": {
                    "icon": "font-awesome/fa-user-secret",
                    "label": {
                        "en-US": "Endpoint*"
                    },
                    "type": "input",
                    "opts": {
                        "types": [
                            "str",
                            "env",
                            "cred"
                        ]
                    }
                }
            },
            {
                "name": "env-prompt-system",
                "type": "str",
                "value": "llm.prompt.system",
                "ui": {
                    "icon": "font-awesome/fa-align-justify",
                    "label": {
                        "en-US": "System*"
                    },
                    "type": "input",
                    "opts": {
                        "types": [
                            "str",
                            "env"
                        ]
                    }
                }
            },
            {
                "name": "env-prompt-user",
                "type": "str",
                "value": "llm.prompt.user",
                "ui": {
                    "icon": "font-awesome/fa-user",
                    "label": {
                        "en-US": "User*"
                    },
                    "type": "input",
                    "opts": {
                        "types": [
                            "str",
                            "env"
                        ]
                    }
                }
            },
            {
                "name": "env-prompt-context",
                "type": "str",
                "value": "llm.prompt.context",
                "ui": {
                    "icon": "font-awesome/fa-commenting-o",
                    "label": {
                        "en-US": "Context*"
                    },
                    "type": "input",
                    "opts": {
                        "types": [
                            "str",
                            "json",
                            "env"
                        ]
                    }
                }
            },
            {
                "name": "env-prompt-history",
                "type": "str",
                "value": "llm.prompt.history",
                "ui": {
                    "icon": "font-awesome/fa-comments",
                    "label": {
                        "en-US": "History"
                    },
                    "type": "input",
                    "opts": {
                        "types": [
                            "str",
                            "env"
                        ]
                    }
                }
            },
            {
                "name": "env-tools",
                "type": "str",
                "value": "llm.tools",
                "ui": {
                    "icon": "font-awesome/fa-puzzle-piece",
                    "label": {
                        "en-US": "Tools*"
                    },
                    "type": "input",
                    "opts": {
                        "types": [
                            "str",
                            "env"
                        ]
                    }
                }
            },
            {
                "name": "env-options-format",
                "type": "str",
                "value": "",
                "ui": {
                    "icon": "font-awesome/fa-code",
                    "label": {
                        "en-US": "Format"
                    },
                    "type": "select",
                    "opts": {
                        "opts": [
                            {
                                "l": {
                                    "en-US": "String"
                                },
                                "v": "string"
                            },
                            {
                                "l": {
                                    "en-US": "JSON"
                                },
                                "v": "json"
                            }
                        ]
                    }
                }
            }
        ],
        "meta": {
            "module": "node-red-contrib-moonshots-llm",
            "type": "moonshots-llm-client",
            "version": "1.0.0",
            "author": "Jean-Philippe Encausse <jp@encausse.net>",
            "desc": "Wrapper on Azure OpenAI Client",
            "keywords": "Moonshots, NodeRED, LLM, OpenAI",
            "license": "MIT"
        },
        "color": "#9ff6fd",
        "outputLabels": [
            "An error occured",
            "Query external tools",
            "The last answer"
        ],
        "icon": "node-red/light.svg",
        "status": {
            "x": 1020,
            "y": 100,
            "wires": [
                {
                    "id": "dc06a33807fbc2c9",
                    "port": 0
                },
                {
                    "id": "7323eaa5ee9f25fe",
                    "port": 0
                },
                {
                    "id": "eb54f860c1c6b73b",
                    "port": 0
                },
                {
                    "id": "25cd096fb552a05f",
                    "port": 0
                }
            ]
        }
    },
    {
        "id": "05c84f2facf6ca69",
        "type": "function",
        "z": "f36dcac84e162f71",
        "name": "Query OpenAI",
        "func": "let t0 = Date.now()\nlet model = msg.llm.model\ntry { \n    //node.warn(msg.llm)\n    let result = await model.client.getChatCompletions(\n        model.id, \n        msg.llm.chat, \n        msg.llm.options\n    );\n    msg.llm.result = result;\n    //node.warn(msg.llm)\n\n} catch(err){\n    msg.llm.error = { err };\n    return node.send([msg, undefined], false);\n}\n\n// Debug duration of API calls\nmsg.llm.debug.duration = (Date.now() - t0); \n\n// Handle result assume there is only 1\nlet choices = msg.llm.result.choices\nif (!choices || choices.length < 1){\n    msg.llm.error = { err : 'No LLM choices' }\n    return node.send([msg, undefined],false);\n}\n\nreturn node.send([undefined, msg],false);",
        "outputs": 2,
        "timeout": 0,
        "noerr": 0,
        "initialize": "",
        "finalize": "",
        "libs": [],
        "x": 400,
        "y": 260,
        "wires": [
            [
                "a17776677d47af2d"
            ],
            [
                "2a6fb2bb3cebd59c"
            ]
        ],
        "icon": "node-red/link-out.svg"
    },
    {
        "id": "2a6fb2bb3cebd59c",
        "type": "switch",
        "z": "f36dcac84e162f71",
        "name": "LLM Response",
        "property": "llm.result.choices.0.finishReason",
        "propertyType": "msg",
        "rules": [
            {
                "t": "eq",
                "v": "tool_calls",
                "vt": "str"
            },
            {
                "t": "eq",
                "v": "stop",
                "vt": "str"
            },
            {
                "t": "else"
            }
        ],
        "checkall": "false",
        "repair": false,
        "outputs": 3,
        "x": 620,
        "y": 260,
        "wires": [
            [
                "709d8be2580c8069",
                "eb54f860c1c6b73b"
            ],
            [
                "183522ee54d78303"
            ],
            [
                "66c40d0557a2d953"
            ]
        ]
    },
    {
        "id": "66c40d0557a2d953",
        "type": "debug",
        "z": "f36dcac84e162f71",
        "name": "Unpredicted Results",
        "active": true,
        "tosidebar": true,
        "console": false,
        "tostatus": false,
        "complete": "llm.result",
        "targetType": "msg",
        "statusVal": "",
        "statusType": "auto",
        "x": 860,
        "y": 300,
        "wires": []
    },
    {
        "id": "709d8be2580c8069",
        "type": "function",
        "z": "f36dcac84e162f71",
        "name": "LLM : toolCalls",
        "func": "// Store the QUERY from LLM\nlet message = msg.llm.result.choices[0].message;\nmsg.llm.toolCalls = []\nmsg.llm.toolCalls.push(message)\n\n// Store the current Tool\nlet tool = message.toolCalls[0]\nmsg.llm.tool = tool\n\n// Store the current Callback\nmsg.llm.toolCallback = (content) => {\n    msg.llm.toolResults = content\n    msg.llm.toolCalls.push({ role: \"tool\", content: JSON.stringify(content), toolCallId: tool.id })\n    node.send([msg, undefined], false)\n}\n\n// Forward the Callback external\nnode.send([undefined, msg], false)\n",
        "outputs": 2,
        "timeout": 0,
        "noerr": 0,
        "initialize": "",
        "finalize": "",
        "libs": [],
        "x": 840,
        "y": 220,
        "wires": [
            [
                "e53048bc94a0acf2"
            ],
            []
        ],
        "icon": "node-red/link-out.svg"
    },
    {
        "id": "38daa06fe33eeb29",
        "type": "function",
        "z": "f36dcac84e162f71",
        "name": "Initialize Prompt",
        "func": "let MOONSHOTS = global.get('__MOONSHOTS__') || {}\n\nmsg.llm = msg.llm || {}\nmsg.llm.prompt  = msg.llm.prompt  || {} // store various prompt items\nmsg.llm.debug   = msg.llm.debug   || {} // store debug data\nmsg.llm.options = msg.llm.options || {} // store configuration \nmsg.llm.model   = msg.llm.model   || {} // store model client\n\n// System Prompt ----------\nmsg.llm.prompt.system = MOONSHOTS.UTIL.getMessageProperty(msg, env.get(\"env-prompt-system\"))\n\n\n// Context Prompt ----------\nlet llm_context = MOONSHOTS.UTIL.getMessageProperty(msg, env.get(\"env-prompt-context\"))\nif (llm_context && llm_context != env.get(\"env-prompt-context\")){ // Prevent to get the default value\n    msg.llm.prompt.context = [\n        { role: \"user\", content: \"Retrieve context of the chat\" }, \n        { role: \"assistant\", content: llm_context }\n    ]\n}\n\n// History Prompt ----------\nlet llm_history = MOONSHOTS.UTIL.getMessageProperty(msg, env.get(\"env-prompt-history\"))\nif (llm_history && typeof llm_history === 'object'){\n    msg.llm.prompt.history = llm_history\n}\n\n// User Prompt ----------\nlet llm_user = MOONSHOTS.UTIL.getMessageProperty(msg, env.get(\"env-prompt-user\"))\nif (llm_user && typeof llm_user === 'string'){ \n    msg.llm.prompt.user = { role: \"user\", content: llm_user }\n}\n\nreturn node.send(msg,false);\n",
        "outputs": 1,
        "timeout": 0,
        "noerr": 0,
        "initialize": "",
        "finalize": "",
        "libs": [],
        "x": 200,
        "y": 100,
        "wires": [
            [
                "3fae92382b52fb3e"
            ]
        ],
        "icon": "node-red/link-out.svg"
    },
    {
        "id": "3fae92382b52fb3e",
        "type": "function",
        "z": "f36dcac84e162f71",
        "name": "Build Chat Flow",
        "func": "\nlet history = msg.llm.prompt.history = msg.llm.prompt.history || []\nlet chat = msg.llm.chat = []\n\nif (msg.llm.prompt.system) {\n    chat.push({ role: \"system\", content: msg.llm.prompt.system })\n}\n\nif (msg.llm.prompt.context) {\n    chat.push.apply(chat, msg.llm.prompt.context);\n    // delete msg.llm.context;\n}\n\n// Push previous history messages if any\nif (msg.llm.prompt.history) {\n    chat.push.apply(chat, msg.llm.prompt.history);\n}\n\n// Append User message if any\nif (msg.llm.prompt.user) {\n    chat.push(msg.llm.prompt.user)\n    history.push(msg.llm.prompt.user)\n    delete msg.llm.prompt.user;\n}\n\n// Append Custom message if any\nif (msg.llm.prompt.custom) {\n    chat.push(msg.llm.prompt.custom);\n    delete msg.llm.prompt.custom;\n}\n\nreturn node.send(msg,false);",
        "outputs": 1,
        "timeout": 0,
        "noerr": 0,
        "initialize": "",
        "finalize": "",
        "libs": [],
        "x": 200,
        "y": 140,
        "wires": [
            [
                "e53048bc94a0acf2"
            ]
        ],
        "icon": "node-red/link-out.svg"
    },
    {
        "id": "a17776677d47af2d",
        "type": "function",
        "z": "f36dcac84e162f71",
        "name": "Errors",
        "func": "//node.warn(msg.llm);\nreturn msg;",
        "outputs": 1,
        "timeout": 0,
        "noerr": 0,
        "initialize": "",
        "finalize": "",
        "libs": [],
        "x": 650,
        "y": 40,
        "wires": [
            [
                "7323eaa5ee9f25fe"
            ]
        ],
        "icon": "font-awesome/fa-bug"
    },
    {
        "id": "3266227f52496b4b",
        "type": "function",
        "z": "f36dcac84e162f71",
        "name": "Build Options",
        "func": "let MOONSHOTS = global.get('__MOONSHOTS__') || {}\n\n// Build Options\nlet options = msg.llm.options = {\n    \"temperature\": 0.7,\n    \"top_p\": 0.95,\n    \"frequency_penalty\": 0,\n    \"presence_penalty\": 0,\n    \"max_tokens\": 800\n};\n\n// Declare LLM Tools Array ----------\nlet llm_tools = MOONSHOTS.UTIL.getMessageProperty(msg, env.get(\"env-tools\"))\nif (llm_tools){\n    options.tools = msg.llm.tools\n}\n\n// Handle JSON format\n// Require a compatible model AND JSON in the prompt\nlet llm_format = MOONSHOTS.UTIL.getMessageProperty(msg, env.get(\"env-options-format\"))\nif (llm_format == \"json\"){\n    options.responseFormat = { \"type\": \"json_object\" }\n}\n\nreturn node.send(msg, false);",
        "outputs": 1,
        "timeout": 0,
        "noerr": 0,
        "initialize": "",
        "finalize": "",
        "libs": [],
        "x": 210,
        "y": 220,
        "wires": [
            [
                "b1486d503bec1fc2"
            ]
        ],
        "icon": "node-red/link-out.svg"
    },
    {
        "id": "b1486d503bec1fc2",
        "type": "function",
        "z": "f36dcac84e162f71",
        "name": "Build Tools CB",
        "func": "let chat = msg.llm.chat\n\n// Push tool calls\nif (msg.llm.toolCalls){\n    chat.push.apply(chat, msg.llm.toolCalls);\n    //delete msg.llm.toolCalls;\n}\n\nreturn node.send(msg,false);",
        "outputs": 1,
        "timeout": 0,
        "noerr": 0,
        "initialize": "",
        "finalize": "",
        "libs": [],
        "x": 200,
        "y": 260,
        "wires": [
            [
                "05c84f2facf6ca69",
                "dc06a33807fbc2c9"
            ]
        ],
        "icon": "node-red/link-out.svg"
    },
    {
        "id": "183522ee54d78303",
        "type": "function",
        "z": "f36dcac84e162f71",
        "name": "Build Response",
        "func": "let result   = msg.llm.result;\nlet choices  = msg.llm.result.choices\nlet response = choices[0].message.content\n\n// Initialize history juste in case\nmsg.llm.prompt.history = msg.llm.prompt.history || []\n\n// Push + Backup + Exit\nlet options = msg.llm.options\nif (!options.responseFormat || options.responseFormat.type != \"json_object\") {\n    msg.llm.prompt.history.push({ role: \"assistant\", content: response })\n    msg.llm.response = response\n    return node.send(msg, false);\n}\n\n// Should never be called with format\nif (response.startsWith('```json')) {\n    response = response.substring(7, response.length - 3).trim()\n}\n\n// Try to parse JSON responses\ntry {\n    response = JSON.parse(response)\n} catch (ex) {\n    msg.llm.error = { err: 'JSON Parsing error', response, ex }\n    response = { text: response } // We ask for a JSON object\n}\n\n// By design we store the text response in text variable\nmsg.llm.prompt.history.push({ role: \"assistant\", content: response.text }) \nmsg.llm.response = response\nreturn node.send(msg,false);",
        "outputs": 1,
        "timeout": 0,
        "noerr": 0,
        "initialize": "",
        "finalize": "",
        "libs": [],
        "x": 840,
        "y": 260,
        "wires": [
            [
                "25cd096fb552a05f"
            ]
        ],
        "icon": "node-red/link-out.svg"
    },
    {
        "id": "dc06a33807fbc2c9",
        "type": "change",
        "z": "f36dcac84e162f71",
        "name": "",
        "rules": [
            {
                "t": "set",
                "p": "payload.fill",
                "pt": "msg",
                "to": "blue",
                "tot": "str"
            },
            {
                "t": "set",
                "p": "payload.text",
                "pt": "msg",
                "to": "Call Azure OpenAI",
                "tot": "str"
            }
        ],
        "action": "",
        "property": "",
        "from": "",
        "to": "",
        "reg": false,
        "x": 905,
        "y": 80,
        "wires": [
            []
        ],
        "icon": "font-awesome/fa-wrench",
        "l": false
    },
    {
        "id": "7323eaa5ee9f25fe",
        "type": "change",
        "z": "f36dcac84e162f71",
        "name": "",
        "rules": [
            {
                "t": "set",
                "p": "payload.fill",
                "pt": "msg",
                "to": "red",
                "tot": "str"
            },
            {
                "t": "set",
                "p": "payload.text",
                "pt": "msg",
                "to": "llm.error",
                "tot": "msg"
            }
        ],
        "action": "",
        "property": "",
        "from": "",
        "to": "",
        "reg": false,
        "x": 905,
        "y": 40,
        "wires": [
            []
        ],
        "icon": "font-awesome/fa-wrench",
        "l": false
    },
    {
        "id": "eb54f860c1c6b73b",
        "type": "change",
        "z": "f36dcac84e162f71",
        "name": "",
        "rules": [
            {
                "t": "set",
                "p": "payload.fill",
                "pt": "msg",
                "to": "blue",
                "tot": "str"
            },
            {
                "t": "set",
                "p": "payload.text",
                "pt": "msg",
                "to": "Call LLM Tool",
                "tot": "str"
            }
        ],
        "action": "",
        "property": "",
        "from": "",
        "to": "",
        "reg": false,
        "x": 905,
        "y": 120,
        "wires": [
            []
        ],
        "icon": "font-awesome/fa-wrench",
        "l": false
    },
    {
        "id": "25cd096fb552a05f",
        "type": "change",
        "z": "f36dcac84e162f71",
        "name": "",
        "rules": [
            {
                "t": "set",
                "p": "payload",
                "pt": "msg",
                "to": "{}",
                "tot": "json"
            }
        ],
        "action": "",
        "property": "",
        "from": "",
        "to": "",
        "reg": false,
        "x": 905,
        "y": 160,
        "wires": [
            []
        ],
        "icon": "font-awesome/fa-wrench",
        "l": false
    },
    {
        "id": "e53048bc94a0acf2",
        "type": "subflow:63608c8ffd25253a",
        "z": "f36dcac84e162f71",
        "name": "Build Model",
        "x": 210,
        "y": 180,
        "wires": [
            [
                "a17776677d47af2d"
            ],
            [
                "3266227f52496b4b"
            ]
        ]
    }
]
